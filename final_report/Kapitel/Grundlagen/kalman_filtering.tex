\section{Kalman Filter}
The Kalman filter is a statistical state estimation algorithm that estimates the internal state of the system from the noisy measurements. It was designed by Rudolph E. Kalman in 1960 for discrete time linear systems. It is basically a predictor-corrector type estimator that is optimal in the sense that it minimizes the estimated error covariance. The Kalman filters produces good estimates in presence of modeling errors in system. Since the measurements and the states estimation occur at discrete points of time, it is easily implementable in digital computers. The Kalman filters are extensively used in the area of autonomous and guided navigation \citep{gre01}.

%\subsection{Kalman gain}
The state space representation of discrete time linear system affected by random noise is
\begin{equation}
    \begin{split}
        \label{eq:lin_ss}
        x_{k} &= Ax_{k-1} + Bu_k + Ww_{k-1} \in \Re^{n}\\
        y_k &= Cx_k + Vv_k \in \Re^{p},
    \end{split}
\end{equation}
where the random variables $w_k$,$v_k$\footnote{ The subscript $k$ represents the discrete sampling instances. For example $\hat{x}(t_k) = \hat{x}(k \Delta t) = \hat{x}_k$ represents the state x at $k^{th}$ sampling instance and $\hat{x}_{k+1} = \hat{x}(k \Delta t + \Delta t)$ represents the state at the next $({k+1})^{th}$ sampling instant. $\Delta t$ represents the sampling time} represent the process and measurement noise. Both the random variables are assumed to be zero mean Gaussian white noises. Let $Q$ and $R$ be the covariance of process and measurement noise respectively. $A$ is the system matrix $B$ is the input matrix, $W$ is the process noise matrix, $C$ is the measurement sensitivity matrix and $V$ is the output or measurement noise matrix. The number of states and measurement are given by $n$ and $p$.

The conceptual derivation of the Kalman gain is as follows \citep{gre01} :

The priori error $e_k^{-}$ between actual $x_k$ and estimates $\hat x_k^{-}$ \footnote{The priori state estimates are indicated by the $-$ sign in the superscript of the symbol. For example $\hat x_k^-$ represents a priori state estimate.} projected ahead of time is
 \begin{equation}
	e_k^{-} = x_k - \hat{x}_k^{-},
	\label{eq:kf_err}
\end{equation}
its covariance is
\begin{equation}
	P_k^{-} = E[e_k^{-} \cdot e_K^{-T}].
	\label{eq:kf_P}
\end{equation}

The estimates of states $\hat x_k^-$ and the covariance matirx $P_k^{-}$ projected ahead of time of the system in \ref{eq:lin_ss} is
\begin{equation}
	\begin{split}
	\hat{x}_k^{-} &= A \hat x_{k-1} + B u_k \\
	P_k^- &= A P_{k-1} A^{T} + W Q W^T .
	\end{split}
\end{equation}
The posteriori estimates of state $\hat x_k$ and covariance $P_k$ is
\begin{equation} 
	\label{eq:kf_correct}
	\begin{split}
	\hat{x}_k &= \hat{x}_k^{-} + K_k(y_k - C\hat x_k^{-})\\
    {P}_{k} &= (I - {K}_k {C}) {P}_{k}^{-} (I - {K}_k {C})^{T} + {K}_k {R} {K}_k^{T},
	\end{split}
\end{equation}
where $I$ represents the identity matrix of size $n \times n$.

Let us assume the estimates $\hat x_k$ is equal to the actual states $x_k$, $$\hat{x}_k = x_k.$$. 
Substituting $x_k$ in place of $\hat x_k$ in the Equation \ref{eq:kf_correct} and computing the priori error according to Equation \ref{eq:kf_err} gives
\begin{equation}
	e_k^- =  K_k(y_k - C\hat x_k^-).
\end{equation}

The error covariance $P_k^-$ of the above equation is obtained by substituting it in Equation \ref{eq:kf_P}. The gain $K_k$\footnote{The Kalman gain $K_k$ given in Equation \ref{eq:kf_gain} is one of the solution that minimizes the error. It can be algebraically manipulated.} that minimizes the \emph{trace}\footnote{The trace of a square matrix is the sum of the main diagonal entries of the matrix} of $P_k^-$ is given by
%and equating it to zero $$ \dfdx{trace(P_k^-)}{K} = 0 $$ solving the above equation for \emph{K}. One form of \emph{K} that minimizes Equation \ref{eq:kf_correct}
\begin{equation} \label{eq:kf_gain}
 K_k = P_k^- C^T(C P_k^- C^T + V R V^T)^{-1}.
    \end{equation}

The $K_k$ is the gain that results by assuming $x_k=\hat x_k$. This implies the estimates computed using this gain would be equal to the actual states in ideal case. This is the why the Kalman filter is called as optimal state estimator for linear systems.
%In the Equation \ref{eq:kf_gain} as measurement covariance \emph{R} approaches zero, Kalman gain \emph{K} lays more trust on actual measurement $y_k$. On the other hand if $P_k^-$ approaches zero, predicted measurement $C\hat{x_k}^-$ is trusted more.

\subsection{Extended Kalman filter}
\label{subsec:ekf}
Most of the real world estimation scenarios are non linear in nature. Kalman filter algorithm  being a linear state estimation algorithm cannot be applied to the non linear systems. \emph{NASA Ames} devised a method to apply Kalman filter for non linear systems which is called the Extended Kalman filter(EKF) \citep{ekf85}. In EKF the non linear system is linearised by multivariate Taylor series expansion of the non linear function. 

The discrete time non linear system in state space representation is
\begin{equation}
\label{eq:nl_disc}
\begin{split}
x_{k} &= f(x_{k-1},u_k,w_{k-1}) \in \Re^{n}\\
y_k &= h(x_k,u_k,v_k) \in \Re^{p},
\end{split}
\end{equation}
where $x_k,y_k$ denotes the system's state and output. The process and measurement noises are represented by $w_{k-1},v_k$. The nonlinear function $f(x_{k-1},u_k,w_{k-1})$ relates the previous state $x_k$ to the present state $x_{k-1}$ and the nonlinear function $h(x_k,u_k,v_k)$ relates the state $x_k$ and the output $y_k$. 

The prediction and correction phases of the EKF is given as follows:
\begin{itemize}
\item \textbf{Time update or Predict}
\begin{enumerate}
\item The priori state estimates $\hat x_k^{-}$ is
%In practice the individual values of noise $w_k$ and $v_k$ at each time step \emph{k} is not known. So one can compute the approximated state and measurement vector without them as 
\begin{equation}
	\begin{split}
	\label{eq:ekf_priori}
	\hat{x}_k^- &= f(\hat{x}_{k-1},u_{k},0).\\
	%\hat{y}_k &= h(\hat{x}_k^-,u_{k},0),
	\end{split}
\end{equation}
\item The priori covariance matrix is $P_k^-$ is
\begin{equation} P_k^- = A_kP_{k-1}A_k^T + W_kQ_{k-1}W_k^T \end{equation}
%where $\hat{x}_k^-$ is the called the priori state estimate The priori state estimate is used to compute the measurement $\hat{y}_k$ at time step \emph{k}. For the computation of Kalman gain $K$ in Equation \ref{eq:kf_gain}, $P_k$ and $C_k$  have to be computed. The state covariance matrix $P_k$ is $$P_k = A_k P_{k-1} A_k^T + W_k Q_{k-1} W_k^T. $$ 
where $A_k$ and $W_k$ are obtained by computing the Jacobian matrix of the function $f(x_{k-1},u_k,w_{k-1})$ with respect state $x$ and noise $w$. They are
\begin{equation}
	\begin{split}
	A_k(i,j) &= \left. \dfdx{f_i}{x_j}\right \vert_{\hat{x}_{k-1},u_k,0}  \\
	W_k(i,j) &= \left. \dfdx{f_i}{w_j}\right \vert_{\hat{x}_{k-1},u_k,0}.
   % {\color{Red}\boldsymbol{F}_{k-1}} = \left . \frac{\partial f}{\partial \boldsymbol{x} }    \right \vert _{\hat{\boldsymbol{x}}_{k-1|k-1},\boldsymbol{u}_{k-1}} 
	\end{split}
\end{equation}
\end{enumerate}
\item \textbf{Measurement update or Correct} 
\begin{enumerate}
	\item The Kalman gain $K_k$ is 
		$$K_k = P_k^-C^T(C_kP_k^-C_k^T + V_kR_kV_k^T)^{-1},$$
		where $C_k$ and $V_k$ are obtained by computing the Jacobian matrix of the function $h(x_k,u_k,v_k)$ with respect to states $x$ and the measurements noise $v$. They are 
		\begin{equation}
		\begin{split}
		C_k(i,j) &= \left. \dfdx{h_i}{x_j}\right \vert_{(\hat{x}_k^-,u_k,0)} \\
		V_k(i,j) &= \left. \dfdx{h_i}{v_j}\right \vert_{(\hat{x}_k^-,u_k,0)}.
		\end{split}
		\end{equation}
	\item The posteriori state estimate $\hat x_k$ is
		$$\hat{x}_k = \hat{x}_k^- + K_k(y_k-h(\hat{x}_k^-,u_k,0)).$$
	\item The posteriori state covariance matrix $P_k$ is
			$${P}_{k} = (I - {K}_k {C}) {P}_{k}^{-} (I - {K}_k {C})^{T} + {K}_k {R} {K}_k^{T}.$$
\end{enumerate}
\end{itemize}
\begin{figure}  
\input{Bilder/ekf_work.tex}
\caption{Recursive formulation of EKF}
\label{fig:ekf_blk}
\end{figure}

%The $A_k$ and $C_k$ are the linear equivalent of state and measurement matrices of non linear system described in Equation \ref{eq:nl_disc}. $W_k$ and $V_k$ are the noise correlation matrix of process and measurements. They are computed by taking  Jacobian of $\hat x_k^-$  with respect to \emph{w} and and $\hat y_k$ with respect to \emph{v} in Equation \ref{eq:nl_disc}.

\begin{comment}
The priori state estimates in Equation \ref{eq:ekf_priori} are corrected according to Equation \ref{eq:kf_correct}. The corrected estimate is called the posteriori state estimate. It is given by
\begin{equation}
    \hat{x}_k = \hat{x}_k^- + K_k(y_k-h(\hat{x}_k^-,u_k,0)).
\end{equation}
The correction for the process covariance matrix is given by
\begin{equation}
P_k = (I- K_kC_k)P_k^-.
\end{equation}

\subsubsection{Algorithm}
    The recursive formulation of the discrete time EKF is shown in Figure \ref{fig:ekf_blk}.  For every time step \emph{k}, the time update stage projects the current state estimates ahead of time. The measurement update stage corrects the projected estimate. For the next time step $k+1$ the posteriori estimates are time step $k$ from measurement update sage is used to project the state and its error covariance estimates.

The algorithm for discrete time EKF can be given as two steps.
%\begin{algorithm}{ekf}
%	\STATE{Initialize the estimator's state and covariance}
%\end{algorithm}

\begin{itemize}
    \item \textbf{Time update or Predict}
\begin{equation}
\label{eq:ekf_predict}
\begin{aligned}
&\text{Project the state}\\
&\hat{x}_k^- = f(\hat{x}_{k-1},u_k,0)\\
&\text{Project the error covarience}\\
&P_k^- = A_kP_{k-1}A_k^T + W_kQ_{k-1}W_k^T\\
\end{aligned}
\end{equation}
\item \textbf{Measurement Update or Correct}\\
\begin{equation}
\label{eq:ekf_correct}
\begin{split}
&\text{Compute Kalman gain}\\
&K_k = P_k^-C^T(C_kP_k^-C_k^T + V_kR_kV_k^T)^{-1}\\
&\text{Update the estimate with measurement }y_k\\
&\hat{x}_k = \hat{x}_k^- + K_k(y_k-h(\hat{x}_k^-,u_k,0))\\
&\text{Update the error covariance} \\
&P_k = (I- K_kC_k)P_k^-
\end{split}
\end{equation}
\end{itemize}
\end{comment}
The EKF algorithm is applied for the multi body system model of \emph{Toro} in Chapter \ref{ch:multi_mdl} and for the simplified model of IMU in Chapter \ref{ch:simp_mdl}.

\paragraph{Continuous time EKF}
The discrete time EKF is the most common form used in many practical applications. Sometimes it is be difficult to discretize the continuous time dynamic model of the system. For these applications a continuous time formulation of EKF is necessary. It is given by \citep{gel74}
\begin{equation}
    \label{eq:ekf_con}
    \begin{split}
        \dot {\hat x} &= f(\hat x,u) + K ( y-h(\hat x))\\
        \dot P(t) &= A(t)P(t) + P(t)A(t)^T + Q - P(t)H(t)^TR^{-1}H(t)P(t)\\
        K(t) &= P(t)H(t)^TR^{-1},
    \end{split}
\end{equation}
where $$A(t) = \dfdx{f}{x}(\hat x(t), u(t)), \hspace{2cm} H(t) = \dfdx{h}{x} (\hat x(t), u(t))$$ are the Jacobian of the state and measurement equation. The continuous time EKF is applied for the inverted double pendulum system in Chapter \ref{ch:multi_mdl}.

\subsubsection{Tuning rules}
\label{subsec:tune_ekf}
Tuning of the EKF involves setting the values of the noise covariance matrices $Q,R$. When the measurements noise $v_k$ is bigger than the process noise $w_k$ then the emphasis is laid on the prediction model. On the other hand if the process noise $w_k$ is bigger than the measurement noise $v_k$ then the emphasis is laid on measurements. For instance let us consider a system with one state and one measurement. The EKF can be made to rely on the prediction model by setting the covariance $Q<R$ or the measurements by setting $Q>R$.

The initial convergence of the filter depends upon the initial values of the estimator's state $\hat x_0$ and covariance $P_0$. If a states of the estimated system is not known at the beginning then the initial state covariance $P_0$ is set to very high value to achieve the fast convergence to actual value.

\subsection{Unscented Kalman filter}
The unscented Kalman filter is a new class filter for nonlinear systems. It was first addressed by Julier et.al in 1997 \citep{jul97} as an alternative to EKF. This filter introduces the concept called the unscented transformation(UT). The UT is a method to determine the statistics of a random variable after undergoing non linear transformations.
%This leads to a differnt method to compute the error covairance matrix $P_k^-$.

\subsubsection{Unscented transform}
The basic idea behind the UT is that it is easier to approximate a probability distribution of a random variable transformed by an arbitrary nonlinear function than it to approximate the nonlinear function \citep{jul04}. A set of sigma points are chosen so that their mean and covariance are $\bar x$ and $\Sigma_x$. The sigma points are propogated through a nonlinear function which in turn yield set of transformed points. The statistics computed from the transformed points forms the estimate of nonlinearly transformed mean and covariance. This method is similar to the one used by particle filters. The main difference is this method uses deterministic way to choose the sigma points based on the specific properties.

The set $X$ consists of sigma points $x_i$ of length $2n+1$ and weights associated with the mean and covariance $W_i^m, W_i^c $. It is given by \citep{sim07} 
\begin{equation} 
    \label{eq:ut_sigma}
    \begin{split}
    S &= \{i= 0,1,...,2n:x_i,W_i \} \\
    x_0 &= \bar x \\ 
    x_i &= \bar x + \left[ \sqrt{(n+\lambda)\Sigma_x} \right ]_i \hspace{2cm} i = 1, \cdots ,n \\
    x_i &= \bar x - \left[ \sqrt{(n+\lambda)\Sigma_x} \right ]_{i-n} \hspace{2cm} i = 1, \cdots ,n . 
    \end{split}
\end{equation}
The matrix square root of a positive definite matrix $\Sigma_x$ is computed using Cholesky factorisation. 
a new method to transform the statistics of a random variable
The weights associated to the sigma points are given by 
\begin{equation}
    \label{eq:ut_weights}
    \begin{split}
    W_0^m &= \frac{\lambda}{n+\lambda} \\
    W_0^c &= \frac{\lambda}{n+\lambda} + ( 1 - \alpha^2 + \beta)\\
    W_i^m &= \frac{1}{2(n+\lambda)} \hspace{2cm} i = 1, \cdots, 2n \\
    W_i^c &= \frac{1}{2(n+\lambda)} \hspace{2cm} i = 1, \cdots, 2n ,
    \end{split}
\end{equation} 
where $\lambda$ is a scaling parameter given by $$ \lambda = \alpha^2(n+\kappa)-n. $$ The parameters $\alpha,\beta,\kappa$ are the tuning parameters of the UKF. $W_i^m$ and $W_i^c$ are the weights used for computing the mean and covariance of the random variable.
%To obtain an unbiased estimate the weights $W_i$ must obey the condition $$ \sum \limits_{i=0}^n W_i = 1. $$
The sigma points $x_i$ are applied to the non linear function $$y= f(x).$$ The mean $\bar y$, covariance $\Sigma_y$ and cross covariance $\Sigma_{xy}$ of the resulting sigma points are $y_i$ are given by
\begin{equation}
    \label{eq:ut_mean_cov}
    \begin{split}
        \bar y &= \sum \limits_{i=0}^{2n} W_i^m y_i \\
        \Sigma_y &= \sum \limits_{i=0}^{2n} W_i^c [y_i - \bar y ] [ y_i - \bar y] ^T \\
        \Sigma_{xy} &= \sum \limits_{i=0}^{2n} W_i^c [x_i - \bar x ] [ y_i - \bar y] ^T .
    \end{split}
\end{equation}

The matrix form of the UT equations \ref{eq:ut_sigma},\ref{eq:ut_weights} and \ref{eq:ut_mean_cov} is as follows:
\begin{equation}
        \begin{split}
            X &= [ \bar x \hspace{5mm} \cdots \hspace{5mm} \bar x ]+ \sqrt c [ 0 \hspace{5mm} \sqrt \Sigma_x \hspace{5mm} - \sqrt \Sigma_x ]\\
            Y &= f(X) \\
            \bar y &= Y w_m\\
            \Sigma_y &= Y W Y^T \\
            \Sigma_{xy} &= X W Y^T,
        \end{split}
    \end{equation}
    where $X$ is the matrix of sigma points. $c=\alpha^2(n+\kappa)$, the vector $w_m$ and matrix $W$ are given by
    \begin{equation}
        \begin{split}
        w_m &= [W_m^0 \hspace{5mm} \cdots \hspace{5mm} W_m^{2n} ]\\
        W &= (I-[w_m \hspace{2mm} \cdots \hspace{2mm} w_m])diag(W_c^0 \hspace{2mm} \cdots \hspace{2mm} W_c^{2n}) (I-[w_m \hspace{2mm} \cdots \hspace{2mm} w_m])^T.
        \end{split}
    \end{equation}

\begin{comment}
The UKF have all the steps as in the Kalman Filter. The continuous time non linear system with additive white noises $v(t),w(t)$ is as follows \citep{sim07}:
\begin{equation}
    \label{eq:ukf_nlsys}
    \begin{split}
       \frac{dx}{dt} &= f(x(t),u(t),t) + W(t)w(t) \\
       y(t) &= h(x(t),u(t),t) + V(t)v(t),
    \end{split}
\end{equation} 
where $W(t),V(t)$ are arbitrary time varying matrices independent of $x(t)$ and $y(t)$. For sake of simplicity we assume the white noises $v(t),w(t)$  are uncorrelated, so the matrices $W(t),V(t)$ are assumed as identity matrices. The difference between UKF and EKF is in the method used for computation of the state covariance $P_k$. The state covariance of the priori state estimate $\hat x_k^-$ projected ahead of time is $$ P_k^- = \sum \limits_{i=0}^{2n} W_i^c [\hat x_{i,k}^- - \hat{\bar x}_k^-] [\hat x_{i,k}^- - \hat{\bar x}_k^-]^T + Q. $$
Similarly the covariance matrix $\Sigma_{\hat y_k}$ after projecting the priori estimates through the nonlinear function is $$ \Sigma_{\hat y} = \sum \limits_{i=0}^n W_i^c [\hat{y}_{k,i} - \hat{\bar y}_k ] [ \hat{y}_{k,i} - \hat{\bar y}_k] ^T+R.$$
The cross covariance matrix $\Sigma_{\hat x_k^-, \hat y_k}$ is $$ \Sigma_{\hat x_k^- \hat y_k} = \sum \limits_{i=0}^{2n} W_i^c [\hat x_{k,i}^- - \hat{\bar x}_k^- ] [ \hat y_{k,i} - \hat{\bar y}_k] ^T .$$
The Kalman gain $K_k$ is $$K_k = \Sigma_{\hat{\bar x}_k^- \hat y_k} \Sigma_{\hat y_k}^{-1}.$$
The state and covariance are updated according to the following equations:
$$  \begin{aligned}
    \hat{\bar{x}}_k &= \hat{\bar{x}}_k^- + K_k(y_k - \hat{\bar y}_k)\\
    P_k &= P_k^- - K_k \Sigma_{y,k} K_k^T.
    \end{aligned}
    $$
\end{comment}
%\subsubsection{Algorithm}
The UKF algorithm in matrix form is given in \citep{sim07}. The prediction and correction phases of the UKF is given as follows:
\begin{figure}[H] 
    \centering
    \input{Bilder/ukf_work.tex}
    \caption{Working of UKF}
    \label{fig:ukf_blk}
\end{figure}

\begin{itemize}
	\begin{comment}
    \item \textbf{Unscented transform}
    \begin{equation}
        \begin{split}
            X &= [ \bar x \hspace{5mm} \cdots \hspace{5mm} \bar x ]+ \sqrt c [ 0 \hspace{5mm} \sqrt \Sigma_x \hspace{5mm} - \sqrt \Sigma_x ]\\
            Y &= g(X) \\
            \bar y &= Y w_m\\
            \Sigma_y &= Y W Y^T \\
            \Sigma_{xy} &= X W Y^T,
        \end{split}
    \end{equation}
    where $X$ is the matrix of sigma points. $c=\alpha^2(n+\kappa)$, the vector $w_m$ and matrix $W$ are given by
    \begin{equation}
        \begin{split}
        w_m &= [W_m^0 \hspace{5mm} \cdots \hspace{5mm} W_m^{2n} ]\\
        W &= (I-[w_m \hspace{2mm} \cdots \hspace{2mm} w_m])diag(W_c^0 \hspace{2mm} \cdots \hspace{2mm} W_c^{2n}) (I-[w_m \hspace{2mm} \cdots \hspace{2mm} w_m])^T.
        \end{split}
    \end{equation} 
    \end{comment}
    \item \textbf{Time update or Predict:} 
	%\begin{enumerate}
	 The priori state $\hat{\bar x}^-_k$ and covariance ${\bar P}^-_k$ estimates are computed by projecting them ahead of time. For this purpose a set of sigma points has to be propagated through the nonlinear state projection function $f(x,u,t)$. 
    \begin{itemize}
        \item \textbf{Sigma points:} The sigma points $X_{k-1}$ of the posteriori state estimates $\hat x_{k-1}$ and $P_{k-1}$ from the previous time step is
        \begin{equation}
        X_{k-1} = [ \hat{\bar{x}}_k \hspace{5mm} \cdots \hspace{5mm} \hat{\bar{x}}_k]+ \sqrt c [ 0 \hspace{5mm} \sqrt{P_{k-1}} \hspace{5mm} - \sqrt{P_{k-1}} ].
        \end{equation}

        \item \textbf{Unscented transform:} The sigma points are propagated through the nonlinear state projection function:
        \begin{equation}
        \begin{split}
            \hat X_k = f(X_{k-1},u_k,k-1). \\
        \end{split}
        \end{equation}
        \item \textbf{Estimate projection:}
        The mean $\hat{\bar x}^-_k$ and covariance $\hat{\bar P}^-_k$ of the transformed sigma points $X_k$ is 
        \begin{equation}
        	\begin{split}
            \hat{\bar P}^-_k &= \hat X_k W \hat X_k^T + Q_{k-1} \\
            \hat{\bar x}^-_k &= \hat X_k w_m, \\
            \end{split}
        \end{equation}
    \end{itemize}
        
    \item \textbf{Measurement update or Correct:} The posteriori state $\hat{\bar x}_k$ and covariance ${\bar P}_k$ are obtained by updating the priori estimates $\hat{\bar x}^-_k$ and $\hat{\bar P}^-_k$. The measurement update is performed in the following steps:
    \begin{itemize}
        \item \textbf{Sigma points:} The sigma points from priori state estimates is
        \begin{equation}
        X_k^- = [ \hat{\bar{x}}_k^- \hspace{5mm} \cdots \hspace{5mm} \hat{\bar{x}}_k^-]+ \sqrt c [ 0 \hspace{5mm} \sqrt{P_k^-} \hspace{5mm} - \sqrt{P_k^-} ].
        \end{equation}

        \item \textbf{Unscented transform:} The sigma points $X_k^-$ are propagated through the nonlinear measurement function:
        \begin{equation}
        \hat Y_k = h(X_{k}^-,u_k,k) 
        \end{equation}
        The following statistics are computed from the transformed sigma points $Y_k$:
        \begin{equation}
        \begin{split}
        &\hat{\bar y}_k = \hat Y_k w_m \\
        &\Sigma_{\hat y_k} = \hat Y_k W \hat Y_k^T + R_k \\
        &\Sigma_{\hat x_k^- \hat y_k} = \hat X_k^- W \hat Y_k^T, \\
        \end{split}
        \end{equation}
		where $\hat{\bar y}_k$ ,$\Sigma_{\hat y_k}$, $\Sigma_{\hat x_k^- \hat y_k}$ represents the mean, covariance and cross covariance of the transformed sigma points $Y_k$. 
		\item \textbf{Estimate correction:}	The Kalman gain $K_k$ is given by
  	\begin{equation}
  		 K_k = \Sigma_{\hat{\bar x}_k^- \hat y_k} \Sigma_{\hat y_k}^{-1}.
  	\end{equation}
  	The posteriori state $\hat{\bar x}_k$ and covariance ${\bar P}_k$ estimates is computed as follows:
        \begin{equation}
        \begin{split}       
        \hat{\bar{x}}_k &= \hat{\bar{x}}_k^- + K_k(y_k - \hat{\bar y}_k)\\
        P_k &= P_k^- - K_k \Sigma_{y,k} K_k^T
        \end{split}
        \end{equation}
    \end{itemize}
\end{itemize}

The tuning parameters of UKF are $Q,R,\alpha,\beta,\kappa$. The covariance matrices $Q$ and $R$ is set according to tuning rules for EKF in \ref{subsec:tune_ekf}.
\section{Root mean square error}
The Root mean square error (RMSE) is a method to evaluate the performance of the state estimators. It is a measure of difference between the estimates and the true values observed \citep{hyn06}. It is computed for $n$ different predictions as the square root of mean of the square of difference between original value and estimates.
\begin{equation}
    \label{eq:rmse}
    RMSE = \sqrt{\frac{\sum_{i=1}^n (x_i - \hat x_i)^2}{n}}.
\end{equation}

The states are estimated using different estimators. The RMSE values are the state estimates determines the performance of an estimator. For instance let us consider two estimators $est1$ and $est2$ for estimating the state $x$. If $$ {RMSE}_{est1_{x}} < {RMSE}_{est2_{x}} $$ then the estimator $est1$ performs better in estimating the state x. Lower the RMSE value better the performance of the estimator.

In this thesis the RMSE is used to evaluate the performance of the EKF and UKF. It is also used to evaluate two different models that is suitable for the estimation.
