\section{Kalman Filter}
Kalman filter is a statistical state estimation algorithm which estimates the internal state of the system from the noisy measurements. It was designed by Rudolph E. Kalman in 1960 for discrete time linear systems. It is basically a predictor-corrector type estimator that is optimal in the sense that it minimizes the estimated error covariance. Kalman filters provides good estimates in presence of modelling errors in system. Since the measurements occur and the states are estimated at discrete points of time, it is easily implementable in digital computers. Kalman filters are extensively used in the area of autonomous and guided navigation.

\subsection{Kalman gain}
A discrete time linear system affected by random noise is
\begin{equation}
    \begin{split}
        \label{eq:lin_ss}
        x_{k} &= Ax_{k-1} + Bu_k + w_{k-1}\\
        y_k &= Cx_k + v_k,
    \end{split}
\end{equation}
where the random variables $w_k$,$v_k$ represent the process and measurement noise. Both the random variables are assumed to be zero mean Gaussian white noises. Let $Q,R$ be the covariance of process and measurement noise. The error between the actual and predicted value of the state is \citep{gre01}
\begin{equation}\label{eq:kf_err}
e_k^ = x_k - \hat{x}_k,
\end{equation} 
where $x_k$ represents the true states of the system and $\hat x_k$ \footnote{ The estimated states are represented by hat symbol at the top, inorder to differentiate them from the true states of the system. For example $\hat x_k$ represents the estimated states} represents the estimated states.
The covariance of the error equation or the process covariance is 
\begin{equation}\label{eq:kf_P}
P_k^ = E[e_k {e_k}^T].
\end{equation} Kalman filter corrects its estimate based on the predicted state and measured output data by 
\begin{equation} \label{eq:kf_correct}
\hat{x_k} = \hat{x}_k + K(y_k - H\hat x_k).
\end{equation}
Kalman gain is computed by substituting Equation \ref{eq:kf_correct} in Equation \ref{eq:kf_err} to compute the $e_k^-$. Computed $e_k^-$ is substituted in Equation \ref{eq:kf_P} and the expected values are computed to find the error covariance $P_k^-$. Finally \emph{K} is computed by taking the derivative of trace of $P_k^-$ and equating it to zero $$ \dfdx{trace(P_k^-)}{K} = 0 $$ solving the above equation for \emph{K}. One form of \emph{K} that minimizes Equation \ref{eq:kf_correct}
\begin{equation} \label{eq:kf_gain}
 K_k = P_k^- C^T(C P_k^- C^T + R)^{-1}
    \end{equation}
From the Equation \ref{eq:kf_gain} as measurement covariance \emph{R} approaches zero, Kalman gain \emph{K} lays more trust on actual measurement $y_k$. On the other hand if $P_k^-$ approaches zero, predicted measurement $C\hat{x_k}^-$ is trusted more.

\subsection{Extended Kalman filter}
Most of the real world estimation scenarios are non linear in nature. Kalman filter algorithm  being a linear state estimation algorithm cannot be applied to the non linear systems. \emph{NASA Ames} devised a method to apply Kalman filter for non linear systems which is called the Extended Kalman filter(EKF) \citep{ekf85}. In EKF the non linear system is linearised by multivariate Taylor series expansion of the non linear function. 

The discrete time non linear system in state space representation is
\begin{equation}
\label{eq:nl_disc}
\begin{split}
x_{k} &= f(x_{k-1},u_k,w_{k-1})\\
y_k &= h(x_k,u_k,v_k),
\end{split}
\end{equation}
\emph{x,y} denotes the vector of system's state and output. \emph{w,v} represents the process and measurement covariance noise. The non linear funcion that relates the previous state to the present state is  $f(x_{k-1},u_k,w_{k-1})$ and $h(x_k,u_k,v_k$) is the non linear function that relates the output and state. 

In practice the individual values of noise $w_k$ and $v_k$ at each time step \emph{k} is not known. So one can compute the approximated state and measurement vector without them as 
\begin{equation}
\begin{split}
\label{eq:ekf_priori}
\hat{x}_k^- &= f(\hat{x}_{k-1},u_{k},0)\\
\hat{y}_k &= h(\hat{x}_k^-,u_{k},0),
\end{split}
\end{equation}
where $\hat{x}_k^-$\footnote{The priori state estimates are indicated by the $-$ sign in the superscript of the symbol. For example $\hat x_k^-$ represents the prori state estimate.} is the called the priori state estimate. The priori state estimate is used to compute the measurement $\hat{y}_k$ at time step \emph{k}. For the computation of Kalman gain $K$ in Equation \ref{eq:kf_gain}, $P_k$ and $C_k$  have to be computed. The state covariance matrix $P_k$ is $$P_k = A_k P_{k-1} A_k^T + W_k Q_{k-1} W_k^T. $$ 
    $A_k$ and $C_k$  are the Jacobian matrices that results by taking partial derivative of $\hat x_k^-$ and $\hat y_k$ in Equation \ref{eq:nl_disc} with respect to \emph{x}  at time instant \emph{k}. The $A_k$ and $C_k$ are the linear equivalent of state and measurement matrices of non linear system described in Equation \ref{eq:nl_disc}. $W_k$ and $V_k$ are the noise correlation matrix of process and measurements. They are computed by taking  Jacobian of $\hat x_k^-$  with respect to \emph{w} and and $\hat y_k$ with respect to \emph{v} in Equation \ref{eq:nl_disc}.
\begin{equation}
\begin{split}
A_k(i,j) &= \dfdx{f_i}{x_j}(\hat{x}_{k-1},u_k,w_{k-1})\\
C_k(i,j) &= \dfdx{h_i}{x_j}(\hat{x}_k^-,u_k,v_k)\\
W_k(i,j) &= \dfdx{f_i}{w_j}(\hat{x}_{k-1},u_k,w_{k-1})\\
V_k(i,j) &= \dfdx{h_i}{v_j}(\hat{x}_k^-,u_k,v_k)\\
\end{split}
\end{equation}
The priori state estimates in Equation \ref{eq:ekf_priori} are corrected according to Equation \ref{eq:kf_correct}. The corrected estimate is called the posteriori state estimate. It is given by
\begin{equation}
    \hat{x}_k = \hat{x}_k^- + K_k(y_k-h(\hat{x}_k^-,u_k,0)).
\end{equation}
The correction for the process covariance matrix is given by
\begin{equation}
P_k = (I- K_kC_k)P_k^-.
\end{equation}

\subsubsection{Algorithm}
\begin{figure}  
\tikzset{state/.style={ rectangle,rounded corners, draw=black, very thick, minimum height=2em, inner sep=2pt, text centered,}, }
\begin{tikzpicture}[->,>=stealth']
% Prediction or Time update
 \node[state,
      ] (predict) 
       {\begin{tabular}{c}
       \textbf{Time update:}\\
       \hrulefill\\
       $\begin{matrix}
       \hat{x}_k^- = f(\hat{x}_{k-1},u_k,0) \\
       \hfill \\
       P_k^- = A_k P_{k-1} A_k^T+ W_k Q_{k-1} W_k^T 
       \end{matrix}$
       \end{tabular}
       };
       % Correction or Measurement update
       \node[state,       % layout (defined above)
       text width=6.5cm,  % max text width
       right of=predict,    % Position is to the right of QUERY
       node distance=8.5cm,    % distance to QUERY
       anchor=center] (correct) 
       {\begin{tabular}{c}
       \textbf{Measurement upadate:}\\
       \hrulefill\\
       $\begin{matrix}
       K_k = P_k^-C^T(C_kP_k^-C_k^T + V_kR_kV_k^T)^{-1}\\
       \hfill \\
       \hat{x}_k = \hat{x}_k^- + K_k(y_k-h(\hat{x}_k^-,u_k,0))\\
       \hfill \\
       P_k = (I- K_kC_k)P_k^-
       \end{matrix}$
       \end{tabular}
       };
% draw the paths and and print some Text below/above the graph
\path (predict)     edge[bend left]  node[anchor=south,above]{$\hat x_k^-, P_k^-$} (correct)
(correct)       edge[bend left] node[anchor= north,below] {$\hat x_{k-1}, P_{k-1}$} (predict);
\end{tikzpicture}
\caption{Recursive formulation of EKF}
\label{fig:ekf_blk}
\end{figure}

    The recursive formulation of the discrete time EKF is shown in Figure \ref{fig:ekf_blk}.  For every time step \emph{k}, the time update stage projects the current state estimates ahed of time. The measurement update stage corrects the projected estimate. For the next time step $k+1$ the posteriori estimates are time step $k$ from measurement update sage is used to project the state and its error covariance estimates.

The algorithm for discrete time EKF can be given as two steps.
\begin{itemize}
    \item \textbf{Time update or Predict}
\begin{equation}
\label{eq:ekf_predict}
\begin{aligned}
&\text{Project the state}\\
&\hat{x}_k^- = f(\hat{x}_{k-1},u_k,0)\\
&\text{Project the error covarience}\\
&P_k^- = A_kP_{k-1}A_k^T + W_kQ_{k-1}W_k^T\\
\end{aligned}
\end{equation}
\item \textbf{Measurement Update or Correct}\\
\begin{equation}
\label{eq:ekf_correct}
\begin{split}
&\text{Compute Kalman gain}\\
&K_k = P_k^-C^T(C_kP_k^-C_k^T + V_kR_kV_k^T)^{-1}\\
&\text{Update the estimate with measurement }y_k\\
&\hat{x}_k = \hat{x}_k^- + K_k(y_k-h(\hat{x}_k^-,u_k,0))\\
&\text{Update the error covariance} \\
&P_k = (I- K_kC_k)P_k^-
\end{split}
\end{equation}
\end{itemize}

The EKF algorithm is applied for the multi body system model of \emph{Toro} in Chapter \ref{ch:multi_mdl} and for the simplified model of IMU in Chapter \ref{ch:simp_mdl}.

The continuous time EKF algorithm is given by \citep{gel74}
\begin{equation}
    \label{eq:ekf_con}
    \begin{split}
        \dot {\hat x} &= f(\hat x,u) + K ( y-h(\hat x))\\
        \dot P(t) &= A(t)P(t) + P(t)A(t)^T + Q - P(t)H(t)^TR^{-1}H(t)P(t)\\
        K(t) &= P(t)H(t)^TR^{-1},
    \end{split}
\end{equation}
where $$A(t) = \dfdx{f}{x}(\hat x(t), u(t)), \hspace{2cm} H(t) = \dfdx{h}{x} (\hat x(t), u(t))$$ are the Jacobians of the state and measurement equation. The continuous time EKF is applied for the inverted double pendulum system in Chapter \ref{ch:multi_mdl}.

\subsubsection{Tuning rules:}
\label{subsec:tune_ekf}
Tuning of the EKF involves setting the values of the noise covariance matrices $Q,R$. When the measurements $y$ are noisy then the emphasis is laid on the prediction model. In other words the prediction model is relied more. This is achived by setting the covariances $Q<R$. On the other hand if the estimated process is noisy due to unmodelled dynamics or model mismatch then the measurements are relied more. This is achived by setting $Q>R$.

The initial convergence of the filter depends upon the initial values of the estimator's state $\hat x_0$ and covariance $P_0$. If a states of the estimated system is not known at the beginning then the initial state covariance $P_0$ is set to very high value to achive the fast convergence to actual value.

\subsection{Unscented Kalman filter}
The unscented Kalman filter is a new class filter for nonlinear systems. It was first addressed by Julier et.al in 1997 \citep{jul97} as an alternative to EKF. This filter introduces unscented transformation(UT) to propogate mean and covariance information through non linear transformations.
%This leads to a differnt method to compute the error covairance matrix $P_k^-$.

\subsubsection{Unscented transform}
The basic idea behined the UT was founded on the intuition that it is easier to approximate a probability distribution than it is to approximate an arbitrary nonlinear function \citep{jul04}. A set of sigma points are chosen so that their mean and covariance are $\bar x$ and $\Sigma_x$. The sigma points are applied to a non linear function which in turn yeild set of transformed points. The statistics computed from the transformed points forms the estimate of nonlinearly transformed mean and covariance. This method is similar to the one used by particle filters. The main difference is this method uses determinisic way to choose the sigma points based on the specific properties. 

The set $S$ consists of vectors of length $2n+1$ of sigma points $x_i$ and weights associated with the mean and covariance $W_i^m, W_i^c $. It is given by \citep{sim07} 
\begin{equation} 
    \label{eq:ut_sigma}
    \begin{split}
    S &= \{i= 0,1,...,2n:x_i,W_i \} \\
    x_0 &= \bar x \\ 
    x_i &= \bar x + \left[ \sqrt{(n+\lambda)\Sigma_x} \right ]_i \hspace{2cm} i = 1, \cdots ,n \\
    x_i &= \bar x - \left[ \sqrt{(n+\lambda)\Sigma_x} \right ]_{i-n} \hspace{2cm} i = 1, \cdots ,n . 
    \end{split}
\end{equation}
The matrix square root of a positive definite matrix $\Sigma_x$ is computed using Cholesky factorisation. 

The weights associated to the sigma points are given by 
\begin{equation}
    \label{eq:ut_weights}
    \begin{split}
    W_0^m &= \frac{\lambda}{n+\lambda} \\
    W_0^c &= \frac{\lambda}{n+\lambda} + ( 1 - \alpha^2 + \beta)\\
    W_i^m &= \frac{1}{2(n+\lambda)} \hspace{2cm} i = 1, \cdots, 2n \\
    W_i^c &= \frac{1}{2(n+\lambda)} \hspace{2cm} i = 1, \cdots, 2n ,
    \end{split}
\end{equation} 
where $\lambda$ is a scaling parameter given by $$ \lambda = \alpha^2(n+\kappa)-n. $$ The parameters $\alpha,\beta,\kappa$ are the tuning parameters of the UKF.
%To obtain an unbiased estimate the weights $W_i$ must obey the condition $$ \sum \limits_{i=0}^n W_i = 1. $$
The sigma points $x_i$ are applied to the non linear function $$y= f(x).$$ The mean $\bar y$, covariance $\Sigma_y$ and cross covariance $\Sigma_{xy}$ of the resulting sigma points are $y_i$ are given by
\begin{equation}
    \label{eq:ut_mean_cov}
    \begin{split}
        \bar y &= \sum \limits_{i=0}^{2n} W_i^m y_i \\
        \Sigma_y &= \sum \limits_{i=0}^{2n} W_i^c [y_i - \bar y ] [ y_i - \bar y] ^T \\
        \Sigma_{xy} &= \sum \limits_{i=0}^{2n} W_i^c [x_i - \bar x ] [ y_i - \bar y] ^T .
    \end{split}
\end{equation}

The UKF have all the steps as in the Kalman Filter. The continuous time non linear system with additive white noises $v(t),w(t)$ is as follows \citep{sim07}:
\begin{equation}
    \label{eq:ukf_nlsys}
    \begin{split}
       \frac{dx}{dt} &= f(x(t),u(t),t) + W(t)w(t) \\
       y(t) &= h(x(t),u(t),t) + V(t)v(t),
    \end{split}
\end{equation} 
where $W(t),V(t)$ are arbitrary time varying matrices independent of $x(t)$ and $y(t)$. For sake of simplicity we assume the white noises $v(t),w(t)$  are uncorrelated, so the matrices $W(t),V(t)$ are assmed as identity matrices. The difference between UKF and EKF is in the method used for computation of the state covariance $P_k$. The state covariace of the priori state estimate $\hat x_k^-$ projected ahed of time is $$ P_k^- = \sum \limits_{i=0}^{2n} W_i^c [\hat x_{i,k}^- - \hat{\bar x}_k^-] [\hat x_{i,k}^- - \hat{\bar x}_k^-]^T + Q. $$
Similarly the covariance matrix $\Sigma_{\hat y_k}$ after projecting the priori estimates through the nonlinear function is $$ \Sigma_{\hat y} = \sum \limits_{i=0}^n W_i^c [\hat{y}_{k,i} - \hat{\bar y}_k ] [ \hat{y}_{k,i} - \hat{\bar y}_k] ^T+R.$$
The cross covariance matrix $\Sigma_{\hat x_k^-, \hat y_k}$ is $$ \Sigma_{\hat x_k^- \hat y_k} = \sum \limits_{i=0}^{2n} W_i^c [\hat x_{k,i}^- - \hat{\bar x}_k^- ] [ \hat y_{k,i} - \hat{\bar y}_k] ^T .$$
The Kalman gain $K_k$ is $$K_k = \Sigma_{\hat{\bar x}_k^- \hat y_k} \Sigma_{\hat y_k}^{-1}.$$
The state and covariance are updated according to the following equations:
$$  \begin{aligned}
    \hat{\bar{x}}_k &= \hat{\bar{x}}_k^- + K_k(y_k - \hat{\bar y}_k)\\
    P_k &= P_k^- - K_k \Sigma_{y,k} K_k^T.
    \end{aligned}
    $$
\subsubsection{Algorithm}
The UKF algorithm in matrix form is given in \citep{sim07}. The matrix from UKF is chosen as it is easy to implement in Matlab.
\begin{itemize}
    \item \textbf{Unscented transform}
    \begin{equation}
        \begin{split}
            X &= [ \bar x \hspace{5mm} \cdots \hspace{5mm} \bar x ]+ \sqrt c [ 0 \hspace{5mm} \sqrt \Sigma_x \hspace{5mm} - \sqrt \Sigma_x ]\\
            Y &= g(X) \\
            \bar y &= Y w_m\\
            \Sigma_y &= Y W Y^T \\
            \Sigma_{xy} &= X W Y^T,
        \end{split}
    \end{equation}
    where $X$ is the matrix of sigma points. $c=\alpha^2(n+\kappa)$, the vector $w_m$ and matrix $W$ are given by
    \begin{equation}
        \begin{split}
        w_m &= [W_m^0 \hspace{5mm} \cdots \hspace{5mm} W_m^{2n} ]\\
        W &= (I-[w_m \hspace{2mm} \cdots \hspace{2mm} w_m])diag(W_c^0 \hspace{2mm} \cdots \hspace{2mm} W_c^{2n}) (I-[w_m \hspace{2mm} \cdots \hspace{2mm} w_m])^T.
        \end{split}
    \end{equation} 
    
    \item \textbf{Time update:} Project the state mean state mean $\hat{\bar x}^-_k$ and covariance ${\bar P}^-_k$ ahed of time. The time update is perfomed by the following steps:
    \begin{itemize}
        \item \textbf{Sigma points:} Compute the sigma points from the positeriori state estimates from the previous time step.
        \begin{equation}
        X_{k-1} = [ \hat{\bar{x}}_k \hspace{5mm} \cdots \hspace{5mm} \hat{\bar{x}}_k]+ \sqrt c [ 0 \hspace{5mm} \sqrt{P_{k-1}} \hspace{5mm} - \sqrt{P_{k-1}} ]
        \end{equation}

        \item \textbf{Unscented transform:} Propogate the sigma points through the non linear state projection function and compute their statistics.
        \begin{equation}
        \begin{split}
            &\hat X_k = f(X_{k-1},u_k,k-1) \\
            &\text{Mean of the transformed points }\\
            &\hat{\bar x}^-_k = \hat X_k w_m \\
        \end{split}
        \end{equation}

        \item \textbf{Estimate projection:} Compute the state covariance matrix from the projected estimates.
        \begin{equation}
            \hat{\bar P}^-_k = \hat X_k W \hat X_k^T + Q_{k-1}
        \end{equation}

    \end{itemize}
        
    \item \textbf{Measurement update:} The updated state mean $\hat{\bar x}_k$ and covariance ${\bar P}_k$. The measurement update is performed in the follwoing steps:
    \begin{itemize}
        \item \textbf{Sigma points:} Compute the sigma points from priori state estimates.
        \begin{equation}
        X_k^- = [ \hat{\bar{x}}_k^- \hspace{5mm} \cdots \hspace{5mm} \hat{\bar{x}}_k^-]+ \sqrt c [ 0 \hspace{5mm} \sqrt{P_k^-} \hspace{5mm} - \sqrt{P_k^-} ]
        \end{equation}

        \item \textbf{Unscented transform:} Propogate the sigma points through the measurement function and compute their statistics.
        \begin{equation}
        \begin{split}
        &\hat Y_k = h(X_{k}^-,u_k,k) \\
        &\text{Mean of the transfromed points}\\
        &\hat{\bar y}_k = \hat Y_k w_m \\
        &\text{Covariance of the transfromed points}\\
        &\Sigma_{\hat y_k} = \hat Y_k W \hat Y_k^T + R_k \\
        &\text{Cross covariance of the transformed points}\\
        &\Sigma_{\hat x_k^- \hat y_k} = \hat X_k^- W \hat Y_k^T \\
        \end{split}
        \end{equation}

        \item \textbf{Estimate Correction:} Compute the kalman gain and update the priori estimates.
        \begin{equation}
        \begin{split}
        &\text{Compute Kalman gain}\\
        &K_k = \Sigma_{\hat{\bar x}_k^- \hat y_k} \Sigma_{\hat y_k}^{-1}\\
        &\text{Update the priori state estimates}\\
        &\hat{\bar{x}}_k = \hat{\bar{x}}_k^- + K_k(y_k - \hat{\bar y}_k)\\
        &P_k = P_k^- - K_k \Sigma_{y,k} K_k^T
        \end{split}
        \end{equation}
    \end{itemize}
\end{itemize}
\begin{figure}  
\input{Bilder/ukf_work.tex}
\caption{Working of UKF}
\label{fig:ukf_blk}
\end{figure}

The tuning parameters of UKF are $Q,R,\alpha,\beta,\kappa$. The covariance matrices $Q$ and $R$ is set according to tuning rules for EKF in \ref{subsec:tune_ekf}.
\section{Root mean square error (RMSE)}
Root mean square error (RMSE) is a measure of difference between the estimates and the true values observed \citep{hyn06}. It is computed for $n$ different predictions as the square root of mean of the square of difference between original value and estimates.
\begin{equation}
    \label{eq:rmse}
    RMSE = \sqrt{\frac{\sum_{i=1}^n (x_i - \hat x_i)^2}{n}}.
\end{equation}
Lower the RMSE values better the performance of the estimator.
